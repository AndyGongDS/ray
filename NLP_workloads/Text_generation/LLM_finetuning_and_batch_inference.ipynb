{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model Fine-Tuning and Batch Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_pipeline_full.png\" width=\"100%\" loading=\"lazy\">\n",
    "\n",
    "Welcome to this tutorial notebook, where you'll explore how to leverage [Ray AI Runtime (AIR)](https://docs.ray.io/en/latest/ray-air/getting-started.html) to perform distributed data preprocessing, fine-tuning, hyperparameter tuning, and batch inference using the [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) model applied to the [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) dataset.\n",
    "\n",
    "[FLAN-T5](https://arxiv.org/pdf/2210.11416.pdf) is transformer-based language model based on [Google's T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html) architecture and fine-tuned on instruction data. You will be further training this model on [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), a set of 52k instructions and demonstrations. Through Ray AIR's integration with the Hugging Face hub, these components are easily accessible, and this example can be adapted for use with other similar models.\n",
    "\n",
    "By the end of this tutorial, you'll have a comprehensive understanding of how to harness Ray AIR to efficiently distribute complex machine learning tasks, allowing you to scale your projects easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up imports and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import transformers\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "transformers.set_seed(42)\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 12:02:57,566\tINFO worker.py:1529 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.9.13', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2024-07-20_12-02-54_231222_51877/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2024-07-20_12-02-54_231222_51877/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2024-07-20_12-02-54_231222_51877', 'metrics_export_port': 62410, 'gcs_address': '127.0.0.1:62164', 'address': '127.0.0.1:62164', 'dashboard_agent_listen_port': 52365, 'node_id': '39973a326e805c8b398fe9f89c36845ff2dd80f9b9da4a91214138dd'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calling `ray.init()`, you will initialize a Ray cluster. Follow the link outputted above to open the Ray Dashboard——a vital observability tool for understanding your infrastructure and application."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data ingest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset\n",
    "\n",
    "You will be fine-tuning the model on [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca) to hopefully further refine the question answering and text generation ability of the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from utils import get_random_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 41601\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output', 'text'],\n",
       "        num_rows: 10401\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train\").train_test_split(\n",
    "    test_size=0.2, seed=57\n",
    ")\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>input</th>\n",
       "      <th>output</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Generate an appropriate closing statement.</td>\n",
       "      <td></td>\n",
       "      <td>Thank you for your time and consideration. I hope to hear from you soon.</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nGenerate an appropriate closing statement.\\n\\n### Response:\\nThank you for your time and consideration. I hope to hear from you soon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Describe your ideal work environment.</td>\n",
       "      <td></td>\n",
       "      <td>My ideal work environment is one that is vibrant and supportive. I would want to be surrounded by people who are passionate and motivated, as well as colleagues who encourage and challenge each other. I would also want access to the latest technologies so that I could stay up to date with the latest industry trends. I would also value an open-minded culture that focuses on collaboration, creativity, and innovation, and that emphasizes the importance of work-life balance. Furthermore, I believe a great workplace should prioritize employee growth and development, offering education and mentorship opportunities to its staff.</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDescribe your ideal work environment.\\n\\n### Response:\\nMy ideal work environment is one that is vibrant and supportive. I would want to be surrounded by people who are passionate and motivated, as well as colleagues who encourage and challenge each other. I would also want access to the latest technologies so that I could stay up to date with the latest industry trends. I would also value an open-minded culture that focuses on collaboration, creativity, and innovation, and that emphasizes the importance of work-life balance. Furthermore, I believe a great workplace should prioritize employee growth and development, offering education and mentorship opportunities to its staff.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the meaning of the term \"bill of lading\"?</td>\n",
       "      <td></td>\n",
       "      <td>A bill of lading is a document issued by a carrier to a shipper as a contract of carriage. It is evidence of the details of the shipment, describing the type and quantity of goods being transported, the origin and destination of the goods, the date of shipment, and the parties involved. It also serves as a receipt for goods.</td>\n",
       "      <td>Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nWhat is the meaning of the term \"bill of lading\"?\\n\\n### Response:\\nA bill of lading is a document issued by a carrier to a shipper as a contract of carriage. It is evidence of the details of the shipment, describing the type and quantity of goods being transported, the origin and destination of the goods, the date of shipment, and the parties involved. It also serves as a receipt for goods.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = get_random_elements(dataset=hf_dataset[\"train\"], num_examples=3)\n",
    "display(HTML(df.to_html()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are four feature columns in the dataset:\n",
    "\n",
    "* `instruction` - The original prompt or query such as \"How do we reduce air pollution?\"\n",
    "* `input` - Any additional context that wasn't provided by the instruction.\n",
    "* `output` - A sample generated response as generated by [Open AI's](https://platform.openai.com/docs/models/gpt-3-5) `text-davinci-003`.\n",
    "* `text` - The instruction, input, output, along with an [instructional prefix](https://github.com/tatsu-lab/stanford_alpaca#data-release)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Ray Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(num_blocks=1, num_rows=52002, schema={instruction: string, input: string, output: string, text: string}),\n",
       " 'test': Dataset(num_blocks=1, num_rows=52002, schema={instruction: string, input: string, output: string, text: string})}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_dataset = ray.data.from_huggingface(hf_dataset)\n",
    "ray_dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ray Datasets](https://docs.ray.io/en/master/data/dataset.html#datasets) are the standard method for loading and exchanging data in Ray AIR libraries. They are specifically designed for easy distributed batch preprocessing, and you can easily convert from a Hugging Face dataset to Ray by using [`ray.data.from_huggingface()`](https://docs.ray.io/en/master/data/api/doc/ray.data.from_huggingface.html#ray.data.from_huggingface)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up train and validation Ray datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_DATA = True\n",
    "\n",
    "if SMALL_DATA:\n",
    "    train_dataset = ray_dataset[\"train\"].limit(100)\n",
    "    validation_dataset = ray_dataset[\"test\"].limit(100)\n",
    "else:\n",
    "    train_dataset = ray_dataset[\"train\"]\n",
    "    validation_dataset = ray_dataset[\"test\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the `SMALL_DATA` flag which, when `True`, limits the number of samples used for downstream steps. This is to reduce training time for demonstration purposes. However, if you have more time, it is advised to set this flag to `False` to utilize the full dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed preprocessing\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_pipeline_data.png\" width=\"100%\" loading=\"lazy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.data.preprocessors import BatchMapper\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(batch: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Tokenizes the input and instruction pairs in a batch using the T5 tokenizer\n",
    "    from the Google/flan-t5-base model, and returns a dictionary containing the\n",
    "    encoded inputs and labels.\n",
    "\n",
    "    Args:\n",
    "        batch: A dictionary containing at least two keys, \"instruction\" and\n",
    "        \"input\", whose values are lists of strings.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the encoded inputs and labels, as returned by\n",
    "        the T5 tokenizer.\n",
    "    \"\"\"\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        list(batch[\"instruction\"]),\n",
    "        list(batch[\"input\"]),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "\n",
    "    encoded_inputs[\"labels\"] = encoded_inputs[\"input_ids\"].copy()\n",
    "\n",
    "    return dict(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_preprocessor = BatchMapper(preprocess_function, batch_format=\"pandas\", batch_size=4096)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define a preprocessing function to convert a batch of data from Alpaca to a format that the FLAN-T5 model can accept. [Ray AIR's `BatchMapper`](https://docs.ray.io/en/latest/ray-air/api/doc/ray.data.preprocessors.BatchMapper.html#ray-data-preprocessors-batchmapper) will then map this function onto each incoming batch during the fine-tuning step.\n",
    "\n",
    "Unpacking this function a bit, the most important component is the [tokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer), which is a Hugging Face component associated with the FLAN-T5 model that turns natural language into formatted tokens with the right padding and truncation necessary for training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed finetuning\n",
    "\n",
    "Now you have the dataset prepared, and a batch preprocessor initialized, it is time to configure [Ray AIR's `HuggingFaceTrainer`](https://docs.ray.io/en/master/train/api/doc/ray.train.huggingface.HuggingFaceTrainer.html#ray.train.huggingface.HuggingFaceTrainer) to distribute FLAN-T5 fine-tuning on Alpaca.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_pipeline_finetune.png\" width=\"100%\" loading=\"lazy\">\n",
    "\n",
    "### Ray AIR Distributed Fine-Tuning Flow\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_train.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Each worker node houses a preprocessor copy to process partitioned batches of the Ray Dataset, and then individual model copies train on these batches. PyTorch DDP synchronizes their weights, resulting in an integrated, fine-tuned model.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize training logic for each worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "use_gpu = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting started, set the batch size (use a small number here since training requires a large amount of compute) and specify use of GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer_init_per_worker(\n",
    "    train_dataset: ray.data.Dataset,\n",
    "    eval_dataset: Optional[ray.data.Dataset] = None,\n",
    "    **config,\n",
    ") -> Trainer:\n",
    "    \"\"\"\n",
    "    Initializes a Hugging Face Trainer for training a T5 text generation model.\n",
    "\n",
    "    Args:\n",
    "        train_dataset (ray.data.Dataset): The dataset for training the model.\n",
    "        eval_dataset (ray.data.Dataset, optional): The dataset for evaluating\n",
    "        the model.\n",
    "            Defaults to None.\n",
    "        config: Additional arguments to configure the Trainer.\n",
    "\n",
    "    Returns:\n",
    "        Trainer: A Hugging Face Trainer for training the T5 model.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model_name = \"google/flan-t5-base\"\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        \"flan-t5-base-finetuned-alpaca\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        learning_rate=config.get(\"learning_rate\", 2e-5),\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=config.get(\"epochs\", 4),\n",
    "        weight_decay=config.get(\"weight_decay\", 0.01),\n",
    "        push_to_hub=False,\n",
    "        disable_tqdm=True,\n",
    "    )\n",
    "\n",
    "    hf_trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    return hf_trainer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `trainer_init_per_worker` function creates a Hugging Face Transformers Trainer that will be distributed by Ray using Distributed Data Parallelism (using PyTorch Distributed backend internally). This means that each worker will have its own copy of the model, but operate on different data. At the end of each step, all the workers will sync gradients.\n",
    "\n",
    "Note: The Hugging Face hub offers different versions of [FLAN-T5](https://huggingface.co/docs/transformers/model_doc/flan-t5) with increasing size. Here, the model and associated tokenizer are [\"flan_t5_base\"](https://huggingface.co/google/flan-t5-base), the smallest variant, in order to expedite fine-tuning for demonstration purposes. You can try this notebook with larger models, and you might find [this related tutorial](https://docs.ray.io/en/master/ray-air/examples/gptj_deepspeed_fine_tuning.html#train) helpful if the model does not fit on a single GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.air.config import RunConfig, ScalingConfig, CheckpointConfig\n",
    "from ray.train.huggingface import HuggingFaceTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since you have access to two GPUs, set the number of workers to match in order to utilize the full cluster for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=False),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "        \"evaluation\": validation_dataset,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Ray AIR's HuggingFaceTrainer](https://docs.ray.io/en/latest/train/api/doc/ray.train.huggingface.HuggingFaceTrainer.html?highlight=ray%20air%20hugging%20face%20trainer) integrates with the Hugging Face Transformers library to scale training and fine-tuning across multiple workers, each with its own copy of the Hugging Face `transformers.Trainer` set up in the previous step.\n",
    "\n",
    "Here, you specify the following:\n",
    "* `trainer_init_per_worker` - Training logic copied onto each worker node.\n",
    "* `scaling_config` - Specify how to scale and the hardware to run on.\n",
    "* `datasets` - Which datasets to run training and evaluation on.\n",
    "* `run_config` - Specify checkpointing behavior (how many times to save the model and how to compare between saved models).\n",
    "* `preprocessor` - The same [Ray AIR preprocessor](https://docs.ray.io/en/latest/ray-air/preprocessors.html) defined above used to transform raw data into tokenized batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-07-20 12:03:33</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:24.41        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.1/16.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/12 CPUs, 0/0 GPUs, 0.0/4.39 GiB heap, 0.0/2.0 GiB objects\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                       </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_8e8df_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/andygong/ray_results/HuggingFaceTrainer_2024-07-20_12-03-08/HuggingFaceTrainer_8e8df_00000_0_2024-07-20_12-03-09/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>status  </th><th>loc            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_8e8df_00000</td><td>ERROR   </td><td>127.0.0.1:51935</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   warnings.warn(\n",
      "2024-07-20 12:03:21,019\tWARNING worker.py:1851 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: be86b2281c1605e67d82174710e706379ab32a8601000000 Worker ID: 9b741c8d63fa41e4a272fcb1a370de38d47184b1da9bb6b067da8a39 Node ID: 39973a326e805c8b398fe9f89c36845ff2dd80f9b9da4a91214138dd Worker IP address: 127.0.0.1 Worker port: 60350 Worker PID: 51946 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/air/util/tensor_extensions/arrow.py:73: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   super().__init__(pa.list_(dtype))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m 2024-07-20 12:03:20,989\tWARNING arrow_serialization.py:157 -- Failed to complete optimized serialization of Arrow Table, serialization of column 'input_ids' of type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> failed, so we're falling back to Arrow IPC serialization for the table. Note that this may result in slower serialization and more worker memory utilization. Serialization error:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 149, in _arrow_table_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     reduced_column = _arrow_chunked_array_reduce(column)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 199, in _arrow_chunked_array_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     chunk_payload = PicklableArrayPayload.from_array(chunk)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 245, in from_array\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     return _array_to_array_payload(a)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 339, in _array_to_array_payload\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     raise ValueError(\"Unhandled Arrow array type:\", a.type)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m ValueError: ('Unhandled Arrow array type:', UnknownExtensionType(ListType(list<item: int64>)))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m 2024-07-20 12:03:20,995\tINFO worker.py:763 -- Task failed with retryable exception: TaskID(0953ca568c98fdc7ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2447, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 450, in serialize\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 428, in _serialize_to_msgpack\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     pickle5_serialized_object = self._serialize_to_pickle5(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 390, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 385, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     inband = pickle.dumps(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     cp.dump(obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 627, in dump\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   File \"pyarrow/types.pxi\", line 1828, in pyarrow.lib.PyExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m NotImplementedError: Please implement UnknownExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51946)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   warnings.warn(\n",
      "2024-07-20 12:03:25,407\tWARNING worker.py:1851 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3e6aa44cf9bff4b228f81392cb64a1d19c7c40be01000000 Worker ID: a7e72146ab47f6972196800b60722ccc00738389b7aea2d481596c59 Node ID: 39973a326e805c8b398fe9f89c36845ff2dd80f9b9da4a91214138dd Worker IP address: 127.0.0.1 Worker port: 60364 Worker PID: 51949 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/air/util/tensor_extensions/arrow.py:73: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   super().__init__(pa.list_(dtype))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m 2024-07-20 12:03:25,385\tWARNING arrow_serialization.py:157 -- Failed to complete optimized serialization of Arrow Table, serialization of column 'input_ids' of type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> failed, so we're falling back to Arrow IPC serialization for the table. Note that this may result in slower serialization and more worker memory utilization. Serialization error:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 149, in _arrow_table_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     reduced_column = _arrow_chunked_array_reduce(column)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 199, in _arrow_chunked_array_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     chunk_payload = PicklableArrayPayload.from_array(chunk)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 245, in from_array\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     return _array_to_array_payload(a)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 339, in _array_to_array_payload\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     raise ValueError(\"Unhandled Arrow array type:\", a.type)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m ValueError: ('Unhandled Arrow array type:', UnknownExtensionType(ListType(list<item: int64>)))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m 2024-07-20 12:03:25,390\tINFO worker.py:763 -- Task failed with retryable exception: TaskID(0953ca568c98fdc7ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2447, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 450, in serialize\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 428, in _serialize_to_msgpack\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     pickle5_serialized_object = self._serialize_to_pickle5(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 390, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 385, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     inband = pickle.dumps(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     cp.dump(obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 627, in dump\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m   File \"pyarrow/types.pxi\", line 1828, in pyarrow.lib.PyExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51949)\u001b[0m NotImplementedError: Please implement UnknownExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   warnings.warn(\n",
      "2024-07-20 12:03:29,437\tWARNING worker.py:1851 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 3f5b32c526da4d11b1d9bb12cce5a096507b180001000000 Worker ID: 9256ec667a83ec6e029922d3927ebfdab32eb116be83fe7786e8034e Node ID: 39973a326e805c8b398fe9f89c36845ff2dd80f9b9da4a91214138dd Worker IP address: 127.0.0.1 Worker port: 60375 Worker PID: 51951 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/air/util/tensor_extensions/arrow.py:73: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   super().__init__(pa.list_(dtype))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m 2024-07-20 12:03:29,417\tWARNING arrow_serialization.py:157 -- Failed to complete optimized serialization of Arrow Table, serialization of column 'input_ids' of type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> failed, so we're falling back to Arrow IPC serialization for the table. Note that this may result in slower serialization and more worker memory utilization. Serialization error:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 149, in _arrow_table_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     reduced_column = _arrow_chunked_array_reduce(column)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 199, in _arrow_chunked_array_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     chunk_payload = PicklableArrayPayload.from_array(chunk)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 245, in from_array\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     return _array_to_array_payload(a)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 339, in _array_to_array_payload\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     raise ValueError(\"Unhandled Arrow array type:\", a.type)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m ValueError: ('Unhandled Arrow array type:', UnknownExtensionType(ListType(list<item: int64>)))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m 2024-07-20 12:03:29,422\tINFO worker.py:763 -- Task failed with retryable exception: TaskID(0953ca568c98fdc7ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2447, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 450, in serialize\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 428, in _serialize_to_msgpack\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     pickle5_serialized_object = self._serialize_to_pickle5(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 390, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 385, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     inband = pickle.dumps(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     cp.dump(obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 627, in dump\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m   File \"pyarrow/types.pxi\", line 1828, in pyarrow.lib.PyExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51951)\u001b[0m NotImplementedError: Please implement UnknownExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   warnings.warn(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/air/util/tensor_extensions/arrow.py:73: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   super().__init__(pa.list_(dtype))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:145: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   for column_name in t.column_names:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:146: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   column = t[column_name]\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:198: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   for chunk in ca.chunks:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:151: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   if not _is_dense_union(column.type) and _is_in_test():\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:156: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   if type(column.type) not in _serialization_fallback_set:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:159: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   f\"serialization of column '{column_name}' of type {column.type} \"\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m 2024-07-20 12:03:33,272\tWARNING arrow_serialization.py:157 -- Failed to complete optimized serialization of Arrow Table, serialization of column 'input_ids' of type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> failed, so we're falling back to Arrow IPC serialization for the table. Note that this may result in slower serialization and more worker memory utilization. Serialization error:\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 149, in _arrow_table_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     reduced_column = _arrow_chunked_array_reduce(column)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 199, in _arrow_chunked_array_reduce\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     chunk_payload = PicklableArrayPayload.from_array(chunk)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 245, in from_array\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     return _array_to_array_payload(a)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py\", line 339, in _array_to_array_payload\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     raise ValueError(\"Unhandled Arrow array type:\", a.type)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m ValueError: ('Unhandled Arrow array type:', UnknownExtensionType(ListType(list<item: int64>)))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/arrow_serialization.py:165: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   _serialization_fallback_set.add(type(column.type))\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m In the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m /Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py:627: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m 2024-07-20 12:03:33,277\tINFO worker.py:763 -- Task failed with retryable exception: TaskID(0953ca568c98fdc7ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"python/ray/_raylet.pyx\", line 619, in ray._raylet.execute_dynamic_generator_and_store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"python/ray/_raylet.pyx\", line 2447, in ray._raylet.CoreWorker.store_task_outputs\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 450, in serialize\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     return self._serialize_to_msgpack(value)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 428, in _serialize_to_msgpack\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     pickle5_serialized_object = self._serialize_to_pickle5(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 390, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     raise e\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/serialization.py\", line 385, in _serialize_to_pickle5\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     inband = pickle.dumps(\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 73, in dumps\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     cp.dump(obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/cloudpickle/cloudpickle_fast.py\", line 627, in dump\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m     return Pickler.dump(self, obj)\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m   File \"pyarrow/types.pxi\", line 1828, in pyarrow.lib.PyExtensionType.__reduce__\n",
      "\u001b[2m\u001b[36m(_map_block_split pid=51953)\u001b[0m NotImplementedError: Please implement UnknownExtensionType.__reduce__\n",
      "2024-07-20 12:03:33,294\tWARNING worker.py:1851 -- A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: 2aaf8f947f6cba4770027d35f9ca1fd13a5263e901000000 Worker ID: 729f1ee0d80e44cdf10111ae16ca8bf973d6d2d8e48872bd543f120a Node ID: 39973a326e805c8b398fe9f89c36845ff2dd80f9b9da4a91214138dd Worker IP address: 127.0.0.1 Worker port: 60388 Worker PID: 51953 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
      "2024-07-20 12:03:33,396\tERROR trial_runner.py:1088 -- Trial HuggingFaceTrainer_8e8df_00000: Error processing event.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/execution/ray_trial_executor.py\", line 1070, in get_next_executor_event\n",
      "    future_result = ray.get(ready_future)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py\", line 2309, in get\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError: \u001b[36mray::_Inner.train()\u001b[39m (pid=51935, ip=127.0.0.1, repr=HuggingFaceTrainer)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 367, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 335, in entrypoint\n",
      "    return self._trainable_func(\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 480, in _trainable_func\n",
      "    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 652, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 389, in train_func\n",
      "    trainer.preprocess_datasets()\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py\", line 309, in preprocess_datasets\n",
      "    self.datasets = self._ingest_spec.preprocess_datasets(\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/_internal/dataset_spec.py\", line 153, in preprocess_datasets\n",
      "    new_datasets[key] = prep.transform(dataset)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 144, in transform\n",
      "    transformed_ds = self._transform(dataset)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 253, in _transform\n",
      "    return dataset.map_batches(\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 617, in map_batches\n",
      "    return Dataset(plan, self._epoch, self._lazy)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 223, in __init__\n",
      "    self._plan.execute(allow_clear_input_blocks=False)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 314, in execute\n",
      "    blocks, stage_info = stage(\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 678, in __call__\n",
      "    blocks = compute._apply(\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 154, in _apply\n",
      "    raise e from None\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 138, in _apply\n",
      "    results = map_bar.fetch_until_complete(refs)\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n",
      "    for ref, result in zip(done, ray.get(done)):\n",
      "ray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name                    </th><th>date               </th><th>experiment_id                   </th><th>hostname                   </th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th>trial_id   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>HuggingFaceTrainer_8e8df_00000</td><td>2024-07-20_12-03-16</td><td>fc3c447650be4cd39f23e68e1b0ccd00</td><td>Zhanyangs-MacBook-Pro.local</td><td>127.0.0.1</td><td style=\"text-align: right;\">51935</td><td style=\"text-align: right;\"> 1721491396</td><td>8e8df_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-20 12:03:33,522\tERROR tune.py:758 -- Trials did not complete: [HuggingFaceTrainer_8e8df_00000]\n",
      "2024-07-20 12:03:33,523\tINFO tune.py:762 -- Total run time: 24.87 seconds (24.40 seconds for the tuning loop).\n"
     ]
    },
    {
     "ename": "RayTaskError",
     "evalue": "\u001b[36mray::_Inner.train()\u001b[39m (pid=51935, ip=127.0.0.1, repr=HuggingFaceTrainer)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 367, in train\n    raise skipped from exception_cause(skipped)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 335, in entrypoint\n    return self._trainable_func(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 480, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 652, in _trainable_func\n    output = fn()\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 389, in train_func\n    trainer.preprocess_datasets()\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py\", line 309, in preprocess_datasets\n    self.datasets = self._ingest_spec.preprocess_datasets(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/_internal/dataset_spec.py\", line 153, in preprocess_datasets\n    new_datasets[key] = prep.transform(dataset)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 144, in transform\n    transformed_ds = self._transform(dataset)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 253, in _transform\n    return dataset.map_batches(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 617, in map_batches\n    return Dataset(plan, self._epoch, self._lazy)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 223, in __init__\n    self._plan.execute(allow_clear_input_blocks=False)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 314, in execute\n    blocks, stage_info = stage(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 678, in __call__\n    blocks = compute._apply(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 154, in _apply\n    raise e from None\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 138, in _apply\n    results = map_bar.fetch_until_complete(refs)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n    for ref, result in zip(done, ray.get(done)):\nray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sc/zm445gtj461388gkngwypfbw0000gn/T/ipykernel_51877/3070752373.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTuneError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTrainingFailedError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError\u001b[0m: \u001b[36mray::_Inner.train()\u001b[39m (pid=51935, ip=127.0.0.1, repr=HuggingFaceTrainer)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 367, in train\n    raise skipped from exception_cause(skipped)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 335, in entrypoint\n    return self._trainable_func(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 480, in _trainable_func\n    super()._trainable_func(self._merged_config, reporter, checkpoint_dir)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 652, in _trainable_func\n    output = fn()\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/base_trainer.py\", line 389, in train_func\n    trainer.preprocess_datasets()\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/data_parallel_trainer.py\", line 309, in preprocess_datasets\n    self.datasets = self._ingest_spec.preprocess_datasets(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/train/_internal/dataset_spec.py\", line 153, in preprocess_datasets\n    new_datasets[key] = prep.transform(dataset)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 144, in transform\n    transformed_ds = self._transform(dataset)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/preprocessor.py\", line 253, in _transform\n    return dataset.map_batches(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 617, in map_batches\n    return Dataset(plan, self._epoch, self._lazy)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/dataset.py\", line 223, in __init__\n    self._plan.execute(allow_clear_input_blocks=False)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 314, in execute\n    blocks, stage_info = stage(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/plan.py\", line 678, in __call__\n    blocks = compute._apply(\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 154, in _apply\n    raise e from None\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/compute.py\", line 138, in _apply\n    results = map_bar.fetch_until_complete(refs)\n  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/data/_internal/progress_bar.py\", line 75, in fetch_until_complete\n    for ref, result in zip(done, ray.get(done)):\nray.exceptions.WorkerCrashedError: The worker died unexpectedly while executing this task. Check python-core-worker-*.log files for more information."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-25 09:25:21,092\tWARNING worker.py:1851 -- The log monitor on node Zhanyangs-MacBook-Pro.local failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/log_monitor.py\", line 520, in <module>\n",
      "    log_monitor.run()\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/log_monitor.py\", line 436, in run\n",
      "    anything_published = self.check_log_files_and_publish_updates()\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/log_monitor.py\", line 339, in check_log_files_and_publish_updates\n",
      "    file_info.reopen_if_necessary()\n",
      "  File \"/Users/andygong/opt/anaconda3/lib/python3.9/site-packages/ray/_private/log_monitor.py\", line 79, in reopen_if_necessary\n",
      "    new_inode = os.stat(self.filename).st_ino\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/tmp/ray/session_2024-07-20_12-02-54_231222_51877/logs/worker-296a63461832eacd77c365049a4bbe77408055973318ad0c879f0be8-01000000-51921.out'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the finetuned model\n",
    "\n",
    "Now that you have a fine-tuned model stored in a Checkpoint, you can retrieve it and test out your own instructions. In a later section, you will implement inference at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = result.checkpoint\n",
    "finetuned_model = checkpoint.get_model(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: You are fetching the fine-tuned FLAN-T5 from the saved [checkpoint object](https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.checkpoint.Checkpoint.html#ray.air.checkpoint.Checkpoint), which requires passing in what kind of model you expect to receive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"How many bees do I have?\"  # Enter your own instruction here.\n",
    "input_query = (\n",
    "    \"I don't have enough bees.\"  # Write additional context for the model here.\n",
    ")\n",
    "\n",
    "inputs = tokenizer(instruction, input_query, return_tensors=\"pt\")\n",
    "outputs = finetuned_model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Optional] Distributed hyperparameter tuning\n",
    "\n",
    "If you would like to tune hyperparameters in pursuit of a better performing model, you can pass the previous `HuggingFaceTrainer` into a [Ray AIR `Tuner`](https://docs.ray.io/en/latest/ray-air/tuner.html) and define the parameter search space to conduct experiments.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_pipeline_tune.png\" width=\"100%\" loading=\"lazy\">\n",
    "\n",
    "### Ray AIR Distributed Hyperparameter Tuning Flow\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_tune.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|To achieve the best configuration for the fine-tuned model, define a Tuner object with a customized search space and behavioral settings for scheduling, scaling, and checkpointing. Running multiple trial experiments using this approach can help converge on the optimal configuration.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers.async_hyperband import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_trials = 4\n",
    "max_tune_epochs = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1\n",
    "use_gpu = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of workers to 1 for each `Trainer` so that hyperparameter tuning can run in parallel rather than sequentially with each trial utilizing all resources per experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = HuggingFaceTrainer(\n",
    "    trainer_init_per_worker=trainer_init_per_worker,\n",
    "    scaling_config=ScalingConfig(num_workers=num_workers, use_gpu=use_gpu),\n",
    "    datasets={\n",
    "        \"train\": train_dataset,\n",
    "        \"evaluation\": validation_dataset,\n",
    "    },\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        ),\n",
    "    ),\n",
    "    preprocessor=batch_preprocessor,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same `HuggingFaceTrainer` created previously, just with a different number of workers for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    trainer,\n",
    "    param_space={\n",
    "        \"trainer_init_config\": {\n",
    "            \"learning_rate\": tune.choice([2e-5, 2e-4, 2e-3, 2e-2]),\n",
    "            \"epochs\": tune.choice([2, 4, 8, max_tune_epochs]),\n",
    "            \"weight_decay\": tune.choice([0.01, 0.1, 1.0, 10.0]),\n",
    "        }\n",
    "    },\n",
    "    tune_config=tune.TuneConfig(\n",
    "        metric=\"eval_loss\",\n",
    "        mode=\"min\",\n",
    "        num_samples=total_num_trials,\n",
    "        scheduler=ASHAScheduler(\n",
    "            max_t=max_tune_epochs,\n",
    "        ),\n",
    "    ),\n",
    "    run_config=RunConfig(\n",
    "        checkpoint_config=CheckpointConfig(\n",
    "            num_to_keep=1,\n",
    "            checkpoint_score_attribute=\"eval_loss\",\n",
    "            checkpoint_score_order=\"min\",\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four major components passed into the Tuner:\n",
    "1. `trainer` - The `HuggingFaceTrainer` with scaling, preprocessing, and fine-tuning logic from earlier.\n",
    "2. `param_space` - The [possibilities of hyperparameters](https://docs.ray.io/en/latest/ray-air/tuner.html#how-to-configure-a-search-space) to tune and search for any given trial.\n",
    "3. `tune_config` - Specify how to compare different experiments, the number of trials, as well as any advanced [search algorithms](https://docs.ray.io/en/latest/tune/key-concepts.html#search-alg-ref) and [schedulers](https://docs.ray.io/en/latest/tune/key-concepts.html#schedulers-ref) like [ASHA](https://openreview.net/forum?id=S1Y7OOlRZ).\n",
    "4. `run_config` - Used to specify checkpointing behavior, custom callbacks, failure/retry configurations, [and more.](https://docs.ray.io/en/latest/ray-air/api/doc/ray.air.RunConfig.html#ray.air.RunConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_grid = tuner.fit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed batch inference\n",
    "\n",
    "Once you have a fine-tuned model, you can apply it to batches of inputs to generate predictions at scale, which is exactly what [Ray AIR's `BatchPredictor`](https://docs.ray.io/en/latest/ray-air/predictors.html#batch-prediction) is designed to facilitate.\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_pipeline_inference.png\" width=\"100%\" loading=\"lazy\">\n",
    "\n",
    "### Ray AIR Distributed Batch Inference Flow\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/NLP_workloads/Text_generation/nlp_batchpredict.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using the best fine-tuned model stored in a Checkpoint object, apply BatchPredictor to new batches of data to generate predictions.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.predictor import Predictor\n",
    "from ray.train.batch_predictor import BatchPredictor\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceModelPredictor(Predictor):\n",
    "    \"\"\"\n",
    "    A Ray Predictor for Hugging Face models that generates text given input data.\n",
    "\n",
    "    Args:\n",
    "        model (transformers.PreTrainedModel): A trained Hugging Face model.\n",
    "        tokenizer (Optional[transformers.PreTrainedTokenizerBase]): A tokenizer\n",
    "        that can tokenize input text.\n",
    "        preprocessor (Optional[Callable]): A function that takes raw input data\n",
    "        and returns tokenized input data.\n",
    "        use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Any,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        preprocessor: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__(preprocessor)\n",
    "        self.model = model\n",
    "        self.use_gpu = use_gpu\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    @classmethod\n",
    "    def from_checkpoint(\n",
    "        cls,\n",
    "        checkpoint: Any,\n",
    "        model_cls: Any,\n",
    "        *,\n",
    "        tokenizer: Optional[Any] = None,\n",
    "        use_gpu: bool = False,\n",
    "        **get_model_kwargs: Any,\n",
    "    ) -> \"HuggingFaceModelPredictor\":\n",
    "        \"\"\"\n",
    "        Create a HuggingFaceModelPredictor from a checkpoint.\n",
    "\n",
    "        Args:\n",
    "            checkpoint (Any): A checkpoint containing a trained Hugging Face model.\n",
    "            model_cls (Any): The type of Hugging Face model to load from the checkpoint.\n",
    "            tokenizer (Optional[Any]): A tokenizer that can tokenize input text.\n",
    "            use_gpu (bool): Whether to use a GPU or CPU for prediction.\n",
    "            **get_model_kwargs (Any): Additional keyword arguments for loading\n",
    "            the Hugging Face model.\n",
    "\n",
    "        Returns:\n",
    "            HuggingFaceModelPredictor: A Ray Predictor for the Hugging Face model.\n",
    "        \"\"\"\n",
    "        if not tokenizer:\n",
    "            tokenizer = AutoTokenizer\n",
    "        if isinstance(tokenizer, type):\n",
    "            tokenizer = checkpoint.get_tokenizer(tokenizer)\n",
    "        return cls(\n",
    "            checkpoint.get_model(model_cls, **get_model_kwargs),\n",
    "            tokenizer=tokenizer,\n",
    "            preprocessor=checkpoint.get_preprocessor(),\n",
    "            use_gpu=use_gpu,\n",
    "        )\n",
    "\n",
    "    def _predict_numpy(\n",
    "        self,\n",
    "        data: Dict[str, Any],\n",
    "        feature_columns: Optional[List[str]] = None,\n",
    "        **generate_kwargs: Any,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generates text given input data.\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, Any]): A dictionary of input data.\n",
    "            feature_columns (Optional[List[str]]): A list of feature column names\n",
    "            to use for prediction.\n",
    "            **generate_kwargs (Any): Additional keyword arguments for generating text.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: A Pandas DataFrame with a single column \"generated_output\"\n",
    "            containing the generated text.\n",
    "        \"\"\"\n",
    "        # we get already tokenized text here because we have the tokenizer as an AIR preprocessor\n",
    "        if feature_columns:\n",
    "            data = {k: v for k, v in data.items() if k in feature_columns}\n",
    "\n",
    "        data = {\n",
    "            k: torch.from_numpy(v).to(device=self.model.device) for k, v in data.items()\n",
    "        }\n",
    "        generate_kwargs = {**data, **generate_kwargs}\n",
    "\n",
    "        outputs = self.model.generate(**generate_kwargs)\n",
    "        return pd.DataFrame(\n",
    "            self.tokenizer.batch_decode(outputs, skip_special_tokens=True),\n",
    "            columns=[\"generated_output\"],\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establish a custom class for prediction, `HugginFaceModelPredictor`, which extends the base Ray AIR [`Predictor`](https://docs.ray.io/en/latest/ray-air/api/doc/ray.train.predictor.Predictor.html?highlight=ray%20air%20predictor) to generate text responses to input instructions:\n",
    "\n",
    "* The predictor takes a trained Hugging Face model, a tokenizer, and a preprocessor (which can be a function that takes raw input data and returns tokenized input data). \n",
    "\n",
    "* `from_checkpoint` creates a `HuggingFaceModelPredictor` from a checkpoint containing a trained Hugging Face model. \n",
    "\n",
    "* `_predict_numpy` generates text given input data in the form of a dictionary, and returns a Pandas DataFrame with a single column \"generated_output\" containing the generated text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = BatchPredictor.from_checkpoint(\n",
    "    checkpoint=result.checkpoint,\n",
    "    predictor_cls=HuggingFaceModelPredictor,\n",
    "    model_cls=T5ForConditionalGeneration,\n",
    "    tokenizer=T5Tokenizer,\n",
    "    use_gpu=use_gpu,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Ray AIR `BatchPredictor` from a [Checkpoint](https://docs.ray.io/en/latest/ray-air/api/checkpoint.html?highlight=checkpoint) and specify the custom predictor, model class, tokenizer, as well as any additional arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run batch inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predictor.predict(\n",
    "    validation_dataset,\n",
    "    num_gpus_per_worker=int(use_gpu),\n",
    "    batch_size=256,\n",
    "    max_new_tokens=128,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display inputs and generated outputs side by side.\n",
    "input_data_pd = validation_dataset.to_pandas()\n",
    "prediction_pd = prediction.to_pandas()\n",
    "\n",
    "input_data_pd.join(prediction_pd, how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray site**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the discussion board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a meetup group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
