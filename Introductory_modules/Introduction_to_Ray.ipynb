{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Ray\n",
    "---\n",
    "(*Suggested Time To Complete: 30 minutes*)\n",
    "\n",
    "Welcome, we're glad to have you along! This module serves as an interactive introduction to Ray, a flexible distributed computing framework built for Python with data science and machine learning practitioners in mind. Before we jump into the structure of this tutorial, let us first unpack the context of where we are coming from, along with the motivation for learning Ray.\n",
    "\n",
    "![Main Map](../_static/assets/Introduction_to_Ray/map.png)\n",
    "\n",
    "*Figure 1*\n",
    "\n",
    "**Context**\n",
    "\n",
    "Today's artificial intelligence (AI) applications require enormous amounts of data to be trained on and machine learning (ML) models tend to grow over time. From consumer-facing products like recommendation systems and photo editing software to enterprise-level use cases like reducing downtime in manufacturing and order-fulfillment optimization, ML systems have become so complex and infrastructure intensive that developers have no option but to distribute execution across multiple machines. However, distributed computing is hard. It requires specialized knowledge about orchestrating clusters of computers together to efficiently schedule tasks and must provide features like fault tolerance when a component fails, high availability to minimize service interruption, and autoscaling to reduce waste.\n",
    "\n",
    "As a data scientist, machine learning pracitioner, developer, or engineer, your contribution may center on building data processing pipelines, training complicated models, running efficient hyperparameter experiments, creating simulations of agents, and/or serving your application to users. In each case, you need to choose a distributed system to support each task, but you don't want to learn a different programming language or toss out your existing toolbox. This is where Ray comes in.\n",
    "\n",
    "**What is Ray?**\n",
    "\n",
    "Ray is an open source, distributed execution framework that allows you to scale AI and machine learning workloads. Our goal is to keep things simple (enabled by a concise core API) so that you can parallelize Python programs on your laptop, cluster, cloud, or even on-premise with minimal code changes. Ray automatically handles all aspects of distributed execution including orchestration, scheduling, fault tolerance, and auto scaling so that you can scale your apps without becoming a distributed systems expert. With a rich ecosystem of libraries and integrations with many important data science tools, Ray lowers the effort needed to scale compute intensive workloads.\n",
    "\n",
    "**Notebook Outline**\n",
    "\n",
    "This first notebook is part of a series where we will discuss the three major **layers** that comprise Ray, namely its core engine, high-level libraries, and ecosystem of integrations. In this first notebook, we will cover:\n",
    "\n",
    "- Introduction to Ray\n",
    "- Part One: Ray Core\n",
    "    - Ray Core Key Concepts\n",
    "    - Example: Cross-Validation on Housing Data\n",
    "        - Sequential Implementation\n",
    "        - Distributed Implementation with Ray\n",
    "    - Summary\n",
    "- Homework\n",
    "- Next Steps\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "To gain the most from this notebook, it helps if you have a working knowledge of Python as well as previous experience with machine learning. The ideal learner has minimal familiarity with Ray and is interested in leveraging Ray's simple distributed computing framework to scale AI and Python workloads.\n",
    "\n",
    "**Learning Goals**\n",
    "\n",
    "Upon completion of this module, you will have a intuition for:\n",
    "1. An Overview of Ray\n",
    "2. Ray Core Key Concepts\n",
    "2. Ray Core Key API Elements\n",
    "3. How to Navigate Next Steps to Start Scaling Workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part One: Ray Core\n",
    "---\n",
    "\n",
    "![Ray Core](../_static/assets/Introduction_to_Ray/Ray_Core.png)\n",
    "\n",
    "*Figure 2*\n",
    "\n",
    "Ray Core is a low-level, distributed computing framework for Python with a concise core API, and you can think of it as the foundation that Ray's data science libraries (Ray AIR) and third-party integrations (Ray Ecosystem) are built on. This simple and general-purpose Python library enables every developer to easily build scalable, distributed systems that run on your laptop, cluster, cloud or Kubernetes.\n",
    "\n",
    "A key strength lies in Ray Core's simple primitives: Tasks, Actors, and Objects.\n",
    "\n",
    "**Tasks:** Ray enables you to designate functions to be executed asychronously on separate Python workers. These asynchronous Ray functions, *tasks*, can specify their resource requirements in terms of CPUs, GPUs, and custom resources which are used by the cluster scheduler to distribute tasks for parallelized execution.\n",
    "\n",
    "**Actors:** What tasks are to functions, actors are to classes. An actor is a stateful worker and methods of the actor are scheduled on that specific worker and can access and mutate the state of that worker. Like tasks, actors support CPU, GPU, and custom resource requirements.\n",
    "\n",
    "**Objects:** In Ray, tasks and actors create and compute on objects, and we refer to these objects as *remote objects* because they can be stored anywhere in a Ray cluster. We use *object references* to refer to them, and they are cached in Ray's distributed shared memory *object store*.\n",
    "\n",
    "Ray sets up and manages clusters of computers so that you can run distributed tasks on them. Let's take some time to practice the key concepts with a hands-on example for each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction to Remote Functions\n",
    "---\n",
    "Let us begin with something that we know, a basic Python function, and use it as context for understanding remote functions. Say we have the following:\n",
    "\n",
    "```python\n",
    "# A simple Python function\n",
    "def quick_nap(x):\n",
    "  time.sleep(1)\n",
    "  return x\n",
    "```\n",
    "To call this function, you might run  `quick_nap(2)` in which case you would wait a second before getting `2` as your output.\n",
    "\n",
    "Now, taking the same function, but adding a `@ray.remote` decorator to it, we have:\n",
    "\n",
    "```python\n",
    "# A Ray remote function\n",
    "@ray.remote\n",
    "def quick_nap(x):\n",
    "  time.sleep(1)\n",
    "  return x\n",
    "```\n",
    "This is a Ray remote function. To run it, you would append the postfix `.remote()` onto this function call, i.e. `quick_nap.remote(2)`. Instead of returning a `2` after 1 second like the previous function, this function will immediately return an `ObjectID`. For example:\n",
    "\n",
    "```\n",
    "ObjectRef(e0dc174c83599034ffffffffffffffffffffffff0100000001000000)\n",
    "```\n",
    "This `ObjectID` is a *promise* of future work, meaning that the actual task of the function is delegated in the background to a worker. In order to access the expected output, you need to call `ray.get()` on the `ObjectID`. Try it out in the coding excercise below!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "In the coding cell below, print both the `ObjectID` and the result when `x = 2` for this remote function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import random\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()\n",
    "\n",
    "def regular_quick_nap(x):\n",
    "  time.sleep(1)\n",
    "  return x\n",
    "\n",
    "@ray.remote\n",
    "def remote_quick_nap(x):\n",
    "  time.sleep(1)\n",
    "  return x\n",
    "\n",
    "obj_id = remote_quick_nap.remote(2)\n",
    "print(\"Object ID = \" + str(obj_id))\n",
    "print(\"Result = \" + str(ray.get(obj_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion Question**\n",
    "\n",
    "Now, the obvious question arises, *why* do we take our ordinary Python function and turn it into a remote function? To see this, think (and experiment below) about if we looped over our functions:\n",
    "\n",
    "```python\n",
    "for i in range(4):\n",
    "  regular_quick_nap(i)\n",
    "\n",
    "for i in range(4):\n",
    "  remote_quick_nap.remote(i)\n",
    "```\n",
    "\n",
    "- About how long do you expect the regular `for` loop to run (in seconds)?\n",
    "- Do you expect the remote `for` loop to run faster or slower relative to the regular loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Serial vs. Parallel Processes**\n",
    "\n",
    "Python scripts by default execute code in a serial manner. This means that if you perform a calculation inside of a `for` loop, each iteration must finish performing the calculation before the next iteration can start. This means that even if your computer has multiple cores, the script can only take advantage of at most a single core.\n",
    "\n",
    "By using Ray, we can transform `for` loops into code that takes advantage of all the cores on our machine by giving each core a set of loop iterations to work on. In other words, we can run the iterations of the loop in *parallel*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serial Implementation\n",
    "%%time\n",
    "results = []\n",
    "\n",
    "for i in range(4):\n",
    "  results.append(regular_quick_nap(i))\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "Let's now perform the same computation asynchronously. Notice here that we create a list of object IDs, append `.remote()` onto the relevant remote task, append each call to our list of IDs, call `.get` on the object IDs outside of the loop, and see a runtime improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallel Implementation\n",
    "start_time = time.time()\n",
    "\n",
    "obj_ids = []\n",
    "\n",
    "for i in range(4):\n",
    "  obj_ids.append(remote_quick_nap.remote(i))\n",
    "parallel_results = ray.get(obj_ids)\n",
    "\n",
    "print(parallel_results)\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Remote Class as a Stateful Actor\n",
    "\n",
    "In this example, we want to keep track of who invoked a particular method; this could be a use case for telemetry data we want to track. Let's use this actor to track method invocation of an actor method. Each instance will track who invoked it and the number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALLERS = [\"A\", \"B\", \"C\"]\n",
    "\n",
    "@ray.remote\n",
    "class MethodStateCounter:\n",
    "    def __init__(self):\n",
    "        self.invokers = {\"A\": 0, \"B\": 0, \"C\": 0}\n",
    "    \n",
    "    def invoke(self, name):\n",
    "        # pretend to do some work here\n",
    "        time.sleep(0.5)\n",
    "        # update times invoked\n",
    "        self.invokers[name] += 1\n",
    "        # return the state of that invoker\n",
    "        return self.invokers[name]\n",
    "        \n",
    "    def get_invoker_state(self, name):\n",
    "        # return the state of the named invoker\n",
    "        return self.invokers[name]\n",
    "    \n",
    "    def get_all_invoker_state(self):\n",
    "        # reeturn the state of all invokers\n",
    "        return self.invokers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of our Actor \n",
    "worker_invoker = MethodStateCounter.remote()\n",
    "worker_invoker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterate and call the `invoke()` method by random callers and keep track of who called it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    name = random.choice(CALLERS)\n",
    "    worker_invoker.invoke.remote(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke a random caller and fetch the value or invocations of a random caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5): \n",
    "    random_name_invoker = random.choice(CALLERS)\n",
    "    times_invoked = ray.get(worker_invoker.invoke.remote(random_name_invoker))\n",
    "    print(f\"Named caller: {random_name_invoker} called {times_invoked}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetch the count of all callers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ray.get(worker_invoker.get_all_invoker_state.remote()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We didn't have to reason about where and how the actors are scheduled. We did not have to worry about the socket connection or IP addresses where these actors reside. All of that is abstracted away from us. All we did is write Python code, using Ray core APIs, convert our classes into distributed stateful services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Housing Prices with `sklearn`\n",
    "***\n",
    "Now that we've warmed-up with some isolated examples using tasks and actors, let's take a look at how we can apply Ray Core's flexible and simple API to scale a bare bones version of a common ML task: cross-validation.\n",
    "\n",
    "Here, we have a dataset of [California Housing](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html) with 20,640 samples and features including `[longitude, latitude, housing_median_age, total_rooms, total_bedrooms, population, households, median_income, median_house_value, ocean_proximity]`. Given that we want to use a linear regression model, we want to assess how the results will generalize to an unseen independent dataset, say, on new housing data coming in this year. To do this, we would try cross-validation which is a model validation technique that resamples different portions of the data to train and test a model on different iterations. After we conduct these trials, we can average the error to get an estimate of the model's predictive performance.\n",
    "\n",
    "However, training the same model multiple times on different subsets of a dataset can take a long time, especially if you're working with a much more complex model and larger dataset. Pictured below in Figure 3, the sequential approach trains each model one after another in a series.\n",
    "\n",
    "![Sequential Timeline](../_static/assets/Introduction_to_Ray/Sequential_Timeline.png)\n",
    "\n",
    "*Figure 3*\n",
    "\n",
    "In this example, we will first implement the sequential approach, then improve it by distributing training with Ray Core, and finally compare the code differences to highlight how minimal the changes are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "\n",
    "num_trials = 100\n",
    "X, y = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 100 Models Sequentially\n",
    "Here, we will define a function that randomly splits our housing dataset into testing and training subsets (in the style of Monte Carlo Cross-Validation, where subsets are generated without replacement and have non-unique subsets from round to round). `sequential()` then fits a model, generates predictions, and returns the R-squared score (closer to 1 = better performance, closer to 0 = worse performance).\n",
    "\n",
    "Then, we'll train 100 models on these random splits, one after another, and finally print out the average of the rounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def sequential():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "\n",
    "    return r2\n",
    "\n",
    "errors_seq = []\n",
    "\n",
    "for i in range (num_trials):\n",
    "    errors_seq.append(sequential())\n",
    "\n",
    "print(sum(errors_seq) / num_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Sequential -> Parallel\n",
    "\n",
    "We just trained 100 linear regression models in a series and averaged their R-squared values in about ~1 second. Let's now leverage Ray to train these models in parallel (where multiple tasks may happen simultaneously) and see a runtime improvement. In Figure 4, you can visually inspect the difference where the scheduler assigns each available worker (in this timeline, we chose `n = 4` workers) a task. The scheduler itself has a nontrivial overhead involved with communicating between workers and other cluster management.\n",
    "\n",
    "![Distributed_Timeline](../_static/assets/Introduction_to_Ray/Distributed_Timeline.png)\n",
    "\n",
    "*Figure 4*\n",
    "\n",
    "With just a few code changes, we will modify our existing Python program to distribute it among *n* number of workers. Of course, this is a lightweight example, but it's illustrative of the kind of user experience you get with Ray Core's lean API.\n",
    "\n",
    "Notice in Figure 5 that we need to use four API calls:\n",
    "\n",
    "1. `ray.init()` - initialize a Ray context\n",
    "2. `@ray.remote` - a decorator that turns functions into tasks and classes into actors\n",
    "3. `.remote()` - postfix to every remote function, remote class declaration, or invocation of a remote class method; returns an `ObjectID` associated with the work to be done\n",
    "4. `ray.get()` - returns an object or list of objects from the object ID\n",
    "\n",
    "You may notice that instead of storing the result of `train.remote()` directly into a list of `errors`, we instead store it in a list called `obj_ids`. Once you run a Ray remote function, it will immediately return an `ObjectID` (or 'Object Reference'). This `ObjectID` is a *promise* of future work, meaning that the task is delegated to a worker, an `ObjectID` is returned while the task executes in the background, and in order to access the expected output, you need to call `ray.get()` on the `ObjectID`\n",
    "\n",
    "![Housing Diff](../_static/assets/Introduction_to_Ray/Housing_Diff.png)\n",
    "\n",
    "*Figure 5*\n",
    "\n",
    "And with just a few lines of difference, we're able to parallelize training without having to concern ourselves with orchestration, fault tolerance, autoscaling, or anything else that requires specialized knowledge of distributed systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train 100 Models in Parallel with Ray\n",
    "To start, we'll import Ray (check out our [installation instructions](https://docs.ray.io/en/latest/ray-overview/installation.html)) and start a Ray cluster on our local machine that can utilize all the cores available on your computer as workers. We use `ray.is_initialized` to allow us to make sure that we only have one Ray cluster active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As illustrated above in Figure 5, we will:\n",
    "1. Add the decorator `@ray.remote` to our function `distributed()` to specify that it is a task to be run remotely. \n",
    "2. Then we call that function as `distributed.remote()` in the `for` loop to append to our list of object ids. \n",
    "3. Finally, we fetch the result outside of the loop to access the final error list (as to not *block* the launching of remote tasks asynchronously) and print out the average R-squared value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@ray.remote\n",
    "def distributed():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    model = LinearRegression()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    r2 = metrics.r2_score(y_test, predictions)\n",
    "\n",
    "    return r2\n",
    "\n",
    "obj_ids = []\n",
    "\n",
    "for i in range (num_trials):\n",
    "    obj_ids.append(distributed.remote())\n",
    "\n",
    "errors_dist = ray.get(obj_ids)\n",
    "\n",
    "print(sum(errors_dist) / num_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you've done it! You have distributed the training of 100 models in a very through round of cross-validation on our California Housing dataset. Compare the runtime for each method of training 100 models. Is this what you expected?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "1. Introduced to Ray Core and Most Popular Workloads\n",
    "2. Key Concepts of Ray Core\n",
    "3. Sequential -> Distributed Training of 100 Models\n",
    "\n",
    "#### [Key Concepts](https://docs.ray.io/en/latest/ray-core/key-concepts.html)\n",
    "- Tasks\n",
    "- Actors\n",
    "- Objects\n",
    "\n",
    "#### [Key API Elements in This Section](https://docs.ray.io/en/latest/ray-core/package-ref.html#python-api)\n",
    "- `ray.init()`\n",
    "- `@ray.remote`\n",
    "- `.remote()`\n",
    "- `ray.get()`\n",
    "- `ray.put()`\n",
    "\n",
    "#### Next\n",
    "Now that we've covered the core engine, let's go up one layer of abstraction to look at a suite of data science libraries build on top of Ray Core to target specific machine learning workloads in the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "---\n",
    "If you would like to practice your new skills further with some in-depth examples beyond the embedded coding excercises, take a look at this list of suggested problems:\n",
    "- [Read About Debugging and Profiling on Ray](https://docs.ray.io/en/latest/ray-core/troubleshooting.html)\n",
    "    - Dig into how to observe Ray work by visualizing tasks in the Ray timeline, profiling using Python's CProfile, understanding crashes and suboptimal performance, and more in this user guide.\n",
    "- [Distribute a Classical Algorithm with Ray](https://github.com/ray-project/hackathon5-algo)\n",
    "    - In this excercise, go to the GitHub repo linked above for details on choosing a classic algorithm implemented in Python, editing the implementation to parallelize the work with Ray, and compare your results against the sequential implementation.\n",
    "\n",
    "\n",
    "# Next Steps\n",
    "---\n",
    "Congratulations! You have completed your first tutorial on an Introduction to Ray and Ray Core! We introduced the three layers of Ray: Core, AIR, and the Ecosystem. In this notebook, we explored Ray Core's key concepts of tasks, actors, and objects along with key API elements through examples. In the next module, we will talk about Ray AI Runtime, a set of native libraries built on top of Ray Core specialized for machine learning workloads.\n",
    "\n",
    "From here, you can learn and get more involved with our active community of developers and researchers by checking out the following resources:\n",
    "- [Ray's \"Getting Started\" Guides](https://docs.ray.io/en/latest/ray-overview/index.html): A collection of QuickStart Guides for every library including installation walkthrough, examples, blogs, talks, and more!\n",
    "- [Official Ray Website](https://www.ray.io/): Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "- [Join the Community on Slack](https://forms.gle/9TSdDYUgxYs8SA9e8): Find friends to discuss your new learnings in our Slack space.\n",
    "- [Use the Discussion Board](https://discuss.ray.io/): Ask questions, follow topics, and view announcements on this community forum.\n",
    "- [Join a Meetup Group](https://www.meetup.com/Bay-Area-Ray-Meetup/): Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "- [Open an Issue](https://github.com/ray-project/ray/issues/new/choose): Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
